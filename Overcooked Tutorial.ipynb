{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked-AI library, including loading and evaluating agents and visualizing trajectories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# Getting started: Training your agent\n",
    "The most convenient way to train an agent is with the [ppo_rllib_client.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/ppo/ppo_rllib_client.py) file, where you can either pass in the arguments through commandline, or you can directly modify the variables you want to change in the file. \n",
    "\n",
    "You can also start an experiment in another python script as done below, which can sometimes be more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60521ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 10:00:22.486568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 10:00:22.593989: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-22 10:00:22.594035: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-03-22 10:00:23.328094: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-22 10:00:23.328172: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-03-22 10:00:23.328180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "2025-03-22 10:00:35.954003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.954092: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.954135: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.954171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.979136: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.979225: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-03-22 10:00:35.982989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 10:00:37.085100: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2025-03-22 10:00:37.555990: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:00:37.658986: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py:422: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if not isinstance(done_, (bool, np.bool, np.bool_)):\n",
      "2025-03-22 10:00:38.324521: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:00:38.421146: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "from human_aware_rl.ppo.ppo_rllib_client import ex\n",
    "# For all the tunable paramters, check out ppo_rllib_client.py file\n",
    "# Note this is not what the configuration should look like for a real experiment\n",
    "config_updates = {\n",
    "    \"results_dir\": \"tutorial_notebook_results/SP\", # can change this whatever directory you want\n",
    "    \"layout_name\": \"cramped_room\",\n",
    "    \"clip_param\": 0.2,\n",
    "    'gamma': 0.9,\n",
    "    'num_training_iters': 10, #this should usually be a lot higher\n",
    "    'num_workers': 1,\n",
    "    'num_gpus': 0,\n",
    "    \"verbose\": False,\n",
    "    'train_batch_size': 800,\n",
    "    'sgd_minibatch_size': 800,\n",
    "    'num_sgd_iter': 1,\n",
    "    \"evaluation_interval\": 2\n",
    "}\n",
    "run = ex.run(config_updates=config_updates, options={\"--loglevel\": \"ERROR\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de10c0",
   "metadata": {},
   "source": [
    "One can check the results of the experiment run by accessing **run.result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d356cb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_sparse_reward': 0.0, 'average_total_reward': 21.43114766237247}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run.result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "In practice, the reward should be much higher if optimized. Checkout the graph in the [README](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/human_aware_rl) in human_aware_rl module for baseline performances.\n",
    "\n",
    "Similarly, you can train BC agents with the [reproduce_bc.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/imitation/reproduce_bc.py) file under the human_aware_rl/imitation directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f493c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /nas/ucb/micah/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle\n",
      "Number of trajectories processed for each layout: {'cramped_room': 14}\n",
      "Epoch 1/10\n",
      "446/446 - 2s - loss: 0.9640 - sparse_categorical_accuracy: 0.7175 - val_loss: 0.8835 - val_sparse_categorical_accuracy: 0.7058 - lr: 0.0010 - 2s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "446/446 - 1s - loss: 0.8487 - sparse_categorical_accuracy: 0.7241 - val_loss: 0.8261 - val_sparse_categorical_accuracy: 0.7042 - lr: 0.0010 - 943ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "446/446 - 1s - loss: 0.8091 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7062 - lr: 0.0010 - 955ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "446/446 - 1s - loss: 0.7919 - sparse_categorical_accuracy: 0.7252 - val_loss: 0.7774 - val_sparse_categorical_accuracy: 0.7046 - lr: 0.0010 - 954ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "446/446 - 1s - loss: 0.7780 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.7726 - val_sparse_categorical_accuracy: 0.7040 - lr: 0.0010 - 1s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "446/446 - 1s - loss: 0.7701 - sparse_categorical_accuracy: 0.7224 - val_loss: 0.7773 - val_sparse_categorical_accuracy: 0.7030 - lr: 0.0010 - 970ms/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "446/446 - 1s - loss: 0.7630 - sparse_categorical_accuracy: 0.7239 - val_loss: 0.7736 - val_sparse_categorical_accuracy: 0.7008 - lr: 0.0010 - 1s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "446/446 - 1s - loss: 0.7579 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7653 - val_sparse_categorical_accuracy: 0.7018 - lr: 0.0010 - 884ms/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "446/446 - 1s - loss: 0.7540 - sparse_categorical_accuracy: 0.7223 - val_loss: 0.7607 - val_sparse_categorical_accuracy: 0.7042 - lr: 0.0010 - 945ms/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "446/446 - 1s - loss: 0.7498 - sparse_categorical_accuracy: 0.7240 - val_loss: 0.7662 - val_sparse_categorical_accuracy: 0.6996 - lr: 0.0010 - 942ms/epoch - 2ms/step\n",
      "Saving bc model at  tutorial_notebook_results/BC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7ef748434fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layout = \"cramped_room\" # any compatible layouts \n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import (\n",
    "    get_bc_params, # get the configuration for BC agents\n",
    "    train_bc_model, # train the BC model\n",
    ")\n",
    "from human_aware_rl.static import (\n",
    "    CLEAN_2019_HUMAN_DATA_TRAIN, # human trajectories\n",
    ")\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"tutorial_notebook_results/BC\", bc_params, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd72b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. To load an agent, you can use the load_agent function in the [rllib.py](https://github.com/HumanCompatibleAI/overcooked_ai/blob/master/src/human_aware_rl/rllib/rllib.py) file. For the purpose of demonstration, I will be loading a local agent, which is also one of the agents included in the web demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844332c3",
   "metadata": {},
   "source": [
    "## 1.1): Loading PPO agent\n",
    "The PPO agents are all trained via the Ray trainer, so to load a trained agent, we can just use the load_agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aeaae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py:422: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if not isinstance(done_, (bool, np.bool, np.bool_)):\n",
      "2025-03-22 10:01:34.937075: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:35.053493: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:35.765484: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:35.869892: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7ef7482faa40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent\n",
    "agent_path = \"tutorial_notebook_results/SP/PPO_cramped_room_True_nw=1_vf=0.000100_es=0.200000_en=0.100000_kl=0.200000_0_2025-03-22_09-51-24znllxx0d/\"\n",
    "# The first argument is the path to the saved trainer\n",
    "# The second argument is the type of agent to load, which only matters if it is not a self-play agent \n",
    "# The third argument is the agent_index, which is not directly related to the training\n",
    "## It is used in creating the RllibAgent class that is used for evaluation\n",
    "ppo_agent = load_agent(agent_path, \"ppo\", 0)\n",
    "ppo_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143edeb6",
   "metadata": {},
   "source": [
    "This function loads an agent from the trainer. The RllibAgent class is a wrapper around the core policy, which simplifies pairing and evaluating different type of agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.2) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94ab2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x7ef7482fa950>,\n",
       " {'eager': True,\n",
       "  'use_lstm': False,\n",
       "  'cell_size': 256,\n",
       "  'data_params': {'layouts': ['cramped_room'],\n",
       "   'check_trajectories': False,\n",
       "   'featurize_states': True,\n",
       "   'data_path': '/nas/ucb/micah/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle'},\n",
       "  'mdp_params': {'layout_name': 'cramped_room', 'old_dynamics': True},\n",
       "  'env_params': {'horizon': 400,\n",
       "   'mlam_params': {'start_orientations': False,\n",
       "    'wait_allowed': False,\n",
       "    'counter_goals': [],\n",
       "    'counter_drop': [],\n",
       "    'counter_pickup': [],\n",
       "    'same_motion_goals': True}},\n",
       "  'mdp_fn_params': {},\n",
       "  'mlp_params': {'num_layers': 2, 'net_arch': [64, 64]},\n",
       "  'training_params': {'epochs': 10,\n",
       "   'validation_split': 0.15,\n",
       "   'batch_size': 64,\n",
       "   'learning_rate': 0.001,\n",
       "   'use_class_weights': False},\n",
       "  'evaluation_params': {'ep_length': 400, 'num_games': 1, 'display': False},\n",
       "  'action_shape': (6,),\n",
       "  'observation_shape': (96,)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"tutorial_notebook_results/BC\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c37a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - tensorflow - OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7ef7a8246b90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(bc_model, bc_params, stochastic=True)\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent = RlLibAgent(bc_policy, 0, base_env.featurize_state_mdp)\n",
    "bc_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c3f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "/nas/ucb/micah/overcooked_ai/.venv/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py:422: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if not isinstance(done_, (bool, np.bool, np.bool_)):\n",
      "2025-03-22 10:01:46.980300: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:47.086265: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:47.780459: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/lr/Assign' id:321 op device:{requested: '', assigned: ''} def:{{{node ppo/lr/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/lr, ppo/lr/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-22 10:01:47.885295: W tensorflow/c/c_api.cc:291] Operation '{name:'ppo/ppo/dense_4/bias/Adam_1/Assign' id:1270 op device:{requested: '', assigned: ''} def:{{{node ppo/ppo/dense_4/bias/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](ppo/ppo/dense_4/bias/Adam_1, ppo/ppo/dense_4/bias/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7ef6f079c910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import load_agent_pair\n",
    "# if we want to load a self-play agent\n",
    "ap_sp = load_agent_pair(agent_path,\"ppo\",\"ppo\")\n",
    "ap_sp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249778ce",
   "metadata": {},
   "source": [
    "This is convenient when the agents trained are not self-play agents. For example, if we have a PPO agent trained with a BC agent, we can load both as an agent pair at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78bb724",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tutorial_notebook_results/config.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bc_agent_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtutorial_notebook_results/BC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ap_bc \u001b[38;5;241m=\u001b[39m \u001b[43mload_agent_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc_agent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ap_bc\n",
      "File \u001b[0;32m/nas/ucb/micah/overcooked_ai/src/human_aware_rl/rllib/rllib.py:876\u001b[0m, in \u001b[0;36mload_agent_pair\u001b[0;34m(save_path, policy_id_0, policy_id_1)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_agent_pair\u001b[39m(save_path, policy_id_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_id_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    872\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    Returns an Overcooked AgentPair object that has as player 0 and player 1 policies with\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;124;03m    ID policy_id_0 and policy_id_1, respectively\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mload_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_agent_pair_from_trainer(trainer, policy_id_0, policy_id_1)\n",
      "File \u001b[0;32m/nas/ucb/micah/overcooked_ai/src/human_aware_rl/rllib/rllib.py:816\u001b[0m, in \u001b[0;36mload_trainer\u001b[0;34m(save_path, true_num_workers)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# Read in params used to create trainer\u001b[39;00m\n\u001b[1;32m    815\u001b[0m config_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(save_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;66;03m# We use dill (instead of pickle) here because we must deserialize functions\u001b[39;00m\n\u001b[1;32m    818\u001b[0m     config \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m true_num_workers:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# Override this param to lower overhead in trainer creation\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tutorial_notebook_results/config.pkl'"
     ]
    }
   ],
   "source": [
    "bc_agent_path = \"tutorial_notebook_results/BC\"\n",
    "ap_bc = load_agent_pair(bc_agent_path,\"ppo\",\"bc\")\n",
    "ap_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0acdeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.agent.AgentPair at 0x7ef6f079f820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap = AgentPair(ppo_agent,bc_agent)\n",
    "ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95787dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.benchmarking.AgentEvaluator at 0x7ef7482fbbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93676beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'env_params': array([{'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1},\n",
       "        {'start_state_fn': None, 'horizon': 400, 'info_level': 0, 'num_mdp': 1}],\n",
       "       dtype=object),\n",
       " 'mdp_params': array([{'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]},\n",
       "        {'layout_name': 'cramped_room', 'terrain': [['X', 'X', 'P', 'X', 'X'], ['O', ' ', ' ', ' ', 'O'], ['X', ' ', ' ', ' ', 'X'], ['X', 'D', 'X', 'S', 'X']], 'start_player_positions': [(1, 2), (3, 1)], 'start_bonus_orders': [], 'rew_shaping_params': {'PLACEMENT_IN_POT_REW': 3, 'DISH_PICKUP_REWARD': 3, 'SOUP_PICKUP_REWARD': 5, 'DISH_DISP_DISTANCE_REW': 0, 'POT_DISTANCE_REW': 0, 'SOUP_DISTANCE_REW': 0}, 'start_all_orders': [{'ingredients': ['onion', 'onion', 'onion']}]}],\n",
       "       dtype=object),\n",
       " 'ep_returns': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'metadatas': {},\n",
       " 'ep_states': array([[<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f079fca0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f079e5f0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f079f580>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f0345e40>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f03461a0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f0346500>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f03471c0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f0346a40>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f0347010>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029ea70>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029eec0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029f310>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029f4c0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029f8b0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6f029f7c0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7a02e4a60>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7a02e4370>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7a02e5330>],\n",
       "        ...,\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7206cace0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7206cae90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef7206cabf0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e87524a0>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e8752890>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e8752dd0>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e8752f80>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e8753130>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e8752fb0>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a2740>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a2b90>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a2fe0>],\n",
       "        [<overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a3190>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a3340>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e86a3490>,\n",
       "         ...,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e85f6e60>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e85f7250>,\n",
       "         <overcooked_ai_py.mdp.overcooked_mdp.OvercookedState object at 0x7ef6e85f7640>]],\n",
       "       dtype=object),\n",
       " 'ep_dones': array([[False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True],\n",
       "        [False, False, False, ..., False, False, True]], dtype=object),\n",
       " 'ep_lengths': array([400, 400, 400, 400, 400, 400, 400, 400, 400, 400]),\n",
       " 'ep_actions': array([[((-1, 0), 'interact'), ((-1, 0), (0, 1)), ((0, 0), (-1, 0)), ...,\n",
       "         ((-1, 0), (0, -1)), ((0, 0), (1, 0)), ((0, 0), (0, 0))],\n",
       "        [((-1, 0), (0, 1)), ('interact', (0, 1)), ((-1, 0), (0, 0)), ...,\n",
       "         ((1, 0), (1, 0)), ((0, 1), (1, 0)), ((0, 1), (0, 1))],\n",
       "        [((-1, 0), (-1, 0)), ('interact', 'interact'), ((-1, 0), (0, -1)),\n",
       "         ..., ((-1, 0), (0, -1)), ((1, 0), (0, 0)), ((0, 1), (-1, 0))],\n",
       "        ...,\n",
       "        [((-1, 0), (1, 0)), ((0, 0), (0, 1)), ((0, 0), (-1, 0)), ...,\n",
       "         ('interact', (0, 1)), ((1, 0), 'interact'), ('interact', (0, 0))],\n",
       "        [('interact', (-1, 0)), ((0, 1), 'interact'),\n",
       "         ('interact', (0, 1)), ..., ((0, 1), (0, -1)),\n",
       "         ('interact', (0, 0)), ('interact', (1, 0))],\n",
       "        [((0, 1), (1, 0)), ('interact', 'interact'), ((0, 0), (0, 0)),\n",
       "         ..., ((0, 1), (1, 0)), ((-1, 0), 'interact'), ((0, 0), (0, 1))]],\n",
       "       dtype=object),\n",
       " 'ep_rewards': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=object),\n",
       " 'ep_infos': array([[{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.1623392 , 0.1630508 , 0.16890894, 0.17439182, 0.16053832,\n",
       "                 0.17077097]], dtype=float32)}, {'action_probs': array([[0.16287003, 0.15844585, 0.17118943, 0.17690448, 0.15579863,\n",
       "                 0.17479159]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16197926, 0.16039927, 0.17061438, 0.17756605, 0.15855333,\n",
       "                 0.17088772]], dtype=float32)}, {'action_probs': array([[0.16313884, 0.1595657 , 0.17233239, 0.17709668, 0.1553399 ,\n",
       "                 0.17252648]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.15993053, 0.16085851, 0.17144653, 0.18253618, 0.15260945,\n",
       "                 0.17261879]], dtype=float32)}, {'action_probs': array([[0.1580302 , 0.15622565, 0.1738706 , 0.1841905 , 0.15395913,\n",
       "                 0.17372395]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.1598108 , 0.16160344, 0.1716468 , 0.18032824, 0.15486619,\n",
       "                 0.1717446 ]], dtype=float32)}, {'action_probs': array([[0.1588376 , 0.1582854 , 0.17497034, 0.18399468, 0.1505695 ,\n",
       "                 0.17334254]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15886615, 0.15968071, 0.17219451, 0.1822429 , 0.15472022,\n",
       "                 0.17229551]], dtype=float32)}, {'action_probs': array([[0.1602402 , 0.1593841 , 0.17339385, 0.18539803, 0.15018778,\n",
       "                 0.17139602]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[46, 69], [185]], 'useful_onion_pickup': [[46, 69], [185]], 'onion_drop': [[67, 195], [186]], 'useful_onion_drop': [[], []], 'potting_onion': [[], []], 'dish_pickup': [[4, 199, 301], [39, 69, 78, 199]], 'useful_dish_pickup': [[], []], 'dish_drop': [[20, 259], [40, 72, 177]], 'useful_dish_drop': [[20, 259], [40]], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[], []], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[], []], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([0, 0])}, 'ep_sparse_r': 0, 'ep_shaped_r': 0, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([0, 0]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16197926, 0.16039927, 0.17061438, 0.17756605, 0.15855333,\n",
       "                 0.17088772]], dtype=float32)}, {'action_probs': array([[0.16313884, 0.1595657 , 0.17233239, 0.17709668, 0.1553399 ,\n",
       "                 0.17252648]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16197926, 0.16039927, 0.17061438, 0.17756605, 0.15855333,\n",
       "                 0.17088772]], dtype=float32)}, {'action_probs': array([[0.16313884, 0.1595657 , 0.17233239, 0.17709668, 0.1553399 ,\n",
       "                 0.17252648]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.16150971, 0.15739854, 0.17216079, 0.18092592, 0.15272772,\n",
       "                 0.17527735]], dtype=float32)}, {'action_probs': array([[0.16083288, 0.15678203, 0.16982326, 0.1880148 , 0.14987774,\n",
       "                 0.17466922]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16056532, 0.15667738, 0.17201836, 0.1833358 , 0.15215969,\n",
       "                 0.17524344]], dtype=float32)}, {'action_probs': array([[0.15802443, 0.15742199, 0.16922013, 0.18876569, 0.1512885 ,\n",
       "                 0.17527924]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15822528, 0.15748751, 0.17339888, 0.18383262, 0.15412685,\n",
       "                 0.17292893]], dtype=float32)}, {'action_probs': array([[0.15921178, 0.1576472 , 0.17033754, 0.18801689, 0.14991488,\n",
       "                 0.17487173]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[16], [94, 126, 145, 277]], 'useful_onion_pickup': [[16], [94, 126, 145, 277]], 'onion_drop': [[28], [95, 137]], 'useful_onion_drop': [[], []], 'potting_onion': [[], [186]], 'dish_pickup': [[62, 121, 128, 201, 238, 247, 253, 289], [105, 211, 213, 238]], 'useful_dish_pickup': [[], []], 'dish_drop': [[80, 122, 200, 229, 239, 248, 286], [113, 212, 220, 270]], 'useful_dish_drop': [[80, 122, 200, 229, 239, 248], [113, 212, 220, 270]], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[], [186]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[], [186]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([0, 3])}, 'ep_sparse_r': 0, 'ep_shaped_r': 3, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([0, 3]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16227834, 0.16232169, 0.16935293, 0.17413795, 0.16104321,\n",
       "                 0.17086588]], dtype=float32)}, {'action_probs': array([[0.16189973, 0.15962976, 0.1701305 , 0.17654435, 0.15904188,\n",
       "                 0.17275378]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16227834, 0.16232169, 0.16935293, 0.17413795, 0.16104321,\n",
       "                 0.17086588]], dtype=float32)}, {'action_probs': array([[0.16189973, 0.15962976, 0.1701305 , 0.17654435, 0.15904188,\n",
       "                 0.17275378]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.1535736 , 0.15580149, 0.17203628, 0.18693896, 0.15164696,\n",
       "                 0.18000278]], dtype=float32)}, {'action_probs': array([[0.15312731, 0.15309116, 0.17424197, 0.18629238, 0.15372464,\n",
       "                 0.17952257]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15425156, 0.15339772, 0.17078793, 0.18610641, 0.15579702,\n",
       "                 0.17965937]], dtype=float32)}, {'action_probs': array([[0.15268444, 0.15038447, 0.1726484 , 0.18872282, 0.15364435,\n",
       "                 0.1819155 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15396078, 0.1526514 , 0.17319334, 0.18596134, 0.15419598,\n",
       "                 0.1800372 ]], dtype=float32)}, {'action_probs': array([[0.1534383 , 0.1527064 , 0.17054729, 0.18942283, 0.15487906,\n",
       "                 0.17900614]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[53, 99, 123, 138, 158, 196, 279], [4, 26, 73, 269]], 'useful_onion_pickup': [[53, 99, 123, 138, 158, 196], [4, 26, 73]], 'onion_drop': [[95, 106, 131, 151], [19, 31]], 'useful_onion_drop': [[], []], 'potting_onion': [[183, 240], [99]], 'dish_pickup': [[11, 20, 49], [102, 182]], 'useful_dish_pickup': [[], []], 'dish_drop': [[17, 41, 50], [178, 242]], 'useful_dish_drop': [[41, 50], []], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[183, 240], [99]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[183, 240], [99]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([6, 3])}, 'ep_sparse_r': 0, 'ep_shaped_r': 9, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([6, 3]), 'ep_length': 400}}],\n",
       "        ...,\n",
       "        [{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16080128, 0.16186069, 0.17070664, 0.17618288, 0.15858963,\n",
       "                 0.17185883]], dtype=float32)}, {'action_probs': array([[0.16204323, 0.16261593, 0.17061092, 0.17643037, 0.15447804,\n",
       "                 0.17382143]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16197926, 0.16039927, 0.17061438, 0.17756605, 0.15855333,\n",
       "                 0.17088772]], dtype=float32)}, {'action_probs': array([[0.16313884, 0.1595657 , 0.17233239, 0.17709668, 0.1553399 ,\n",
       "                 0.17252648]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.16286197, 0.15867832, 0.16835809, 0.17968196, 0.15770264,\n",
       "                 0.17271702]], dtype=float32)}, {'action_probs': array([[0.16237123, 0.15687174, 0.1692933 , 0.1824306 , 0.15662467,\n",
       "                 0.17240852]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16298842, 0.16081528, 0.17009439, 0.1773307 , 0.15710099,\n",
       "                 0.17167023]], dtype=float32)}, {'action_probs': array([[0.16168131, 0.15679151, 0.16888632, 0.18232702, 0.1572555 ,\n",
       "                 0.17305829]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16025929, 0.15560469, 0.17220578, 0.18345937, 0.15310198,\n",
       "                 0.17536889]], dtype=float32)}, {'action_probs': array([[0.16014256, 0.15642364, 0.17074463, 0.1828058 , 0.15565784,\n",
       "                 0.17422554]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[51, 108, 179, 397], [48, 350]], 'useful_onion_pickup': [[51, 108, 179, 397], [48, 350]], 'onion_drop': [[155], [56, 398]], 'useful_onion_drop': [[], []], 'potting_onion': [[103, 385], []], 'dish_pickup': [[13, 32], [96, 131, 196, 218]], 'useful_dish_pickup': [[], []], 'dish_drop': [[29, 33], [124, 192, 202, 339]], 'useful_dish_drop': [[29, 33], []], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[103, 385], []], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[103, 385], []], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([6, 0])}, 'ep_sparse_r': 0, 'ep_shaped_r': 6, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([6, 0]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16092269, 0.16023342, 0.16941634, 0.17624405, 0.16167772,\n",
       "                 0.17150569]], dtype=float32)}, {'action_probs': array([[0.16210791, 0.15858705, 0.16918093, 0.17562182, 0.16091609,\n",
       "                 0.1735861 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.1623293 , 0.1612435 , 0.16958475, 0.17623486, 0.15883368,\n",
       "                 0.17177396]], dtype=float32)}, {'action_probs': array([[0.16151842, 0.15937427, 0.1686882 , 0.17545947, 0.1609162 ,\n",
       "                 0.1740434 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.15869907, 0.15680926, 0.16976228, 0.18236782, 0.15642159,\n",
       "                 0.17594   ]], dtype=float32)}, {'action_probs': array([[0.15731654, 0.15634032, 0.16972028, 0.18378621, 0.15592867,\n",
       "                 0.17690791]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16074032, 0.15602946, 0.1691479 , 0.18132621, 0.15754294,\n",
       "                 0.17521319]], dtype=float32)}, {'action_probs': array([[0.15789399, 0.15644369, 0.16837148, 0.18325059, 0.157765  ,\n",
       "                 0.17627522]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16074032, 0.15602946, 0.1691479 , 0.18132621, 0.15754294,\n",
       "                 0.17521319]], dtype=float32)}, {'action_probs': array([[0.15789399, 0.15644369, 0.16837148, 0.18325059, 0.157765  ,\n",
       "                 0.17627522]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[65, 321], [7, 20, 35, 164, 246, 267, 355]], 'useful_onion_pickup': [[65, 321], [7, 20, 35, 164, 246, 267, 355]], 'onion_drop': [[76], [18, 34, 49, 269, 365]], 'useful_onion_drop': [[], []], 'potting_onion': [[], [208, 264]], 'dish_pickup': [[2, 92, 312], [65, 157, 278, 386]], 'useful_dish_pickup': [[], []], 'dish_drop': [[18, 260, 314], [132, 160, 346]], 'useful_dish_drop': [[314], [132, 160]], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[], [208, 264]], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[], [208, 264]], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([0, 6])}, 'ep_sparse_r': 0, 'ep_shaped_r': 6, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([0, 6]), 'ep_length': 400}}],\n",
       "        [{'agent_infos': [{'action_probs': array([[0.16044214, 0.16076425, 0.17014563, 0.17589368, 0.16023868,\n",
       "                 0.17251562]], dtype=float32)}, {'action_probs': array([[0.16299166, 0.15722479, 0.16982743, 0.17711645, 0.15736254,\n",
       "                 0.17547713]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15948316, 0.15902728, 0.1712314 , 0.1780716 , 0.1583024 ,\n",
       "                 0.17388414]], dtype=float32)}, {'action_probs': array([[0.16246496, 0.16179901, 0.16925995, 0.17648451, 0.1563012 ,\n",
       "                 0.17369042]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.15906811, 0.15726395, 0.16982809, 0.1796061 , 0.1569173 ,\n",
       "                 0.17731644]], dtype=float32)}, {'action_probs': array([[0.16171454, 0.16128732, 0.16824189, 0.17827979, 0.15515633,\n",
       "                 0.17532018]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         ...,\n",
       "         {'agent_infos': [{'action_probs': array([[0.1631158 , 0.15849571, 0.17120782, 0.1826241 , 0.15444121,\n",
       "                 0.1701154 ]], dtype=float32)}, {'action_probs': array([[0.1626232 , 0.15373537, 0.17325418, 0.18292232, 0.1559268 ,\n",
       "                 0.17153817]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16206643, 0.15531006, 0.17192438, 0.1850254 , 0.15363607,\n",
       "                 0.1720377 ]], dtype=float32)}, {'action_probs': array([[0.16290885, 0.1547548 , 0.17357878, 0.18625206, 0.15238535,\n",
       "                 0.17012016]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None},\n",
       "         {'agent_infos': [{'action_probs': array([[0.16252157, 0.156733  , 0.17254008, 0.1829578 , 0.15431635,\n",
       "                 0.17093115]], dtype=float32)}, {'action_probs': array([[0.16419952, 0.15702602, 0.17338566, 0.1850904 , 0.15044029,\n",
       "                 0.1698581 ]], dtype=float32)}], 'sparse_r_by_agent': [0, 0], 'shaped_r_by_agent': [0, 0], 'phi_s': None, 'phi_s_prime': None, 'episode': {'ep_game_stats': {'tomato_pickup': [[], []], 'useful_tomato_pickup': [[], []], 'tomato_drop': [[], []], 'useful_tomato_drop': [[], []], 'potting_tomato': [[], []], 'onion_pickup': [[113], [1, 97, 128, 156, 183]], 'useful_onion_pickup': [[113], [1, 97, 128, 156, 183]], 'onion_drop': [[], [16, 114, 155, 160, 188]], 'useful_onion_drop': [[], []], 'potting_onion': [[136], []], 'dish_pickup': [[1, 64, 76, 105, 148], [205, 261]], 'useful_dish_pickup': [[], []], 'dish_drop': [[52, 74, 83, 107], [230]], 'useful_dish_drop': [[52, 74, 83], [230]], 'soup_pickup': [[], []], 'soup_delivery': [[], []], 'soup_drop': [[], []], 'optimal_onion_potting': [[136], []], 'optimal_tomato_potting': [[], []], 'viable_onion_potting': [[136], []], 'viable_tomato_potting': [[], []], 'catastrophic_onion_potting': [[], []], 'catastrophic_tomato_potting': [[], []], 'useless_onion_potting': [[], []], 'useless_tomato_potting': [[], []], 'cumulative_sparse_rewards_by_agent': array([0, 0]), 'cumulative_shaped_rewards_by_agent': array([3, 0])}, 'ep_sparse_r': 0, 'ep_shaped_r': 3, 'ep_sparse_r_by_agent': array([0, 0]), 'ep_shaped_r_by_agent': array([3, 0]), 'ep_length': 400}}]],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ap_sp: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "# 400: environment timestep horizon, \n",
    "## should not be necessary is the AgentEvaluator is created with a horizon, but good to have for clarity\n",
    "result = ae.evaluate_agent_pair(ap_sp, 10, 400)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fed7df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"ep_returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "264f7842",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap_bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# we can use any AgentPair class, like the ap_bc object we created earlier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# as we can see the performance is not as good as the self-play agents\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m ae\u001b[38;5;241m.\u001b[39mevaluate_agent_pair(\u001b[43map_bc\u001b[49m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m400\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ap_bc' is not defined"
     ]
    }
   ],
   "source": [
    "# we can use any AgentPair class, like the ap_bc object we created earlier\n",
    "# as we can see the performance is not as good as the self-play agents\n",
    "result = ae.evaluate_agent_pair(ap_bc,10,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464d0c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 10/10 [00:09<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d22e1843a145fa8f8b6101aab76b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "# here we use the self-play agentPair created earlier again\n",
    "trajs = ae.evaluate_agent_pair(ap_sp,10,400)\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b62122",
   "metadata": {},
   "source": [
    "This should spawn a window where you can see what the agents are doing at each timestep. You can drag the slider to go forward and backward in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
